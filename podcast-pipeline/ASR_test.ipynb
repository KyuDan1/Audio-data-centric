{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a0b0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/fr20tb/kyudan/miniforge3/envs/dataset/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/mnt/fr20tb/kyudan/miniforge3/envs/dataset/lib/python3.10/site-packages/lightning/fabric/__init__.py:41: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "/mnt/fr20tb/kyudan/miniforge3/envs/dataset/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/fr20tb/kyudan/miniforge3/envs/dataset/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
      "No exporters were provided. This means that no telemetry data will be collected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-02 02:04:04 mixins:184] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-02 02:04:04 modelPT:188] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    skip_missing_manifest_entries: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    text_field: answer\n",
      "    batch_duration: null\n",
      "    use_bucketing: true\n",
      "    bucket_duration_bins: null\n",
      "    bucket_batch_size: null\n",
      "    num_buckets: 30\n",
      "    bucket_buffer_size: 20000\n",
      "    shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-12-02 02:04:04 modelPT:195] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-02 02:04:04 features:306] PADDING: 0\n",
      "[NeMo I 2025-12-02 02:04:09 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-12-02 02:04:09 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-02 02:04:09 label_looping_base:109] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: No `cuda-python` module. Please do `pip install cuda-python>=12.3`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-02 02:04:09 rnnt_models:226] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-02 02:04:09 label_looping_base:109] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: No `cuda-python` module. Please do `pip install cuda-python>=12.3`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-02 02:04:11 save_restore_connector:282] Model EncDecRNNTBPEModel was successfully restored from /mnt/ddn/kyudan/.cache/huggingface/hub/models--nvidia--parakeet-tdt-0.6b-v2/snapshots/48b630d20b000e5ad3735e5378a2d9bde3f80826/parakeet-tdt-0.6b-v2.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6199111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-02 02:04:15--  https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\n",
      "Resolving dldata-public.s3.us-east-2.amazonaws.com (dldata-public.s3.us-east-2.amazonaws.com)... 52.219.94.146, 3.5.133.198, 52.219.177.162, ...\n",
      "Connecting to dldata-public.s3.us-east-2.amazonaws.com (dldata-public.s3.us-east-2.amazonaws.com)|52.219.94.146|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 237964 (232K) [audio/wav]\n",
      "Saving to: ‘2086-149220-0033.wav’\n",
      "\n",
      "2086-149220-0033.wa 100%[===================>] 232.39K   254KB/s    in 0.9s    \n",
      "\n",
      "2025-12-02 02:04:17 (254 KB/s) - ‘2086-149220-0033.wav’ saved [237964/237964]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839b539d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 100%|██████████| 1/1 [00:02<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, I don't wish to see it any more, observed Phebe, turning away her eyes. It is certainly very like the old portrait.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = asr_model.transcribe(['2086-149220-0033.wav'])\n",
    "print(output[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76af41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Hypothesis(score=4257.033203125, y_sequence=tensor([735, 839,  34, 200, 844, 821,   7, 379,  20, 253,  51, 296, 268, 839,\n",
       "         601, 826,  12, 548, 206, 546, 430, 839,   1,  92, 466,   3, 374, 427,\n",
       "          60, 833,  27, 841, 245,  59,  16, 611, 177,  83, 297, 117,   5,   8,\n",
       "          82,  24, 208, 148,  35, 841]), text=\"Well, I don't wish to see it any more, observed Phebe, turning away her eyes. It is certainly very like the old portrait.\", dec_out=None, dec_state=None, timestamp=[], alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5554967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import separate_fast, dnsmos, whisper_asr, silero_vad\n",
    "asr_model_whisper = whisper_asr.load_asr_model(\n",
    "            \"large-v3\",\n",
    "            \"cuda\",\n",
    "            compute_type=\"float16\",\n",
    "            threads=4,\n",
    "            language=\"en\", \n",
    "            \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aadbe84",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VadFreeFasterWhisperPipeline.transcribe() missing 1 required positional argument: 'vad_segments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transcribe \u001b[38;5;241m=\u001b[39m \u001b[43masr_model_whisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2086-149220-0033.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: VadFreeFasterWhisperPipeline.transcribe() missing 1 required positional argument: 'vad_segments'"
     ]
    }
   ],
   "source": [
    "transcribe = asr_model_whisper.transcribe('2086-149220-0033.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2837e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-02 04:19:23 auto_tokenizer:234] 1 special tokens added, resize your model accordingly.\n",
      "[NeMo I 2025-12-02 04:20:01 lora:30] LoRA adapter installed: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='Qwen/Qwen3-1.7B', revision=None, inference_mode=False, r=128, target_modules=['q_proj', 'v_proj'], exclude_modules=None, lora_alpha=256, lora_dropout=0.01, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "[NeMo I 2025-12-02 04:20:01 features:306] PADDING: 0\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "from nemo.collections.speechlm2.models import SALM\n",
    "\n",
    "model = SALM.from_pretrained('nvidia/canary-qwen-2.5b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2a1df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 151643}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, I don't wish to see it any more, observed Phoebe, turning away her eyes. It is certainly very like the old portrait.\n"
     ]
    }
   ],
   "source": [
    "answer_ids = model.generate(\n",
    "    prompts=[\n",
    "        [{\"role\": \"user\", \"content\": f\"Transcribe the following: {model.audio_locator_tag}\", \"audio\": [\"2086-149220-0033.wav\"]}]\n",
    "    ],\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "print(model.tokenizer.ids_to_text(answer_ids[0].cpu()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714465a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
