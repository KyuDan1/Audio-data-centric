{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gotk7cpq7p8",
   "source": "# ASR Testing Notebook\n\nThis notebook contains various ASR (Automatic Speech Recognition) model tests including:\n- NeMo ASR (Parakeet TDT)\n- Whisper (OpenAI)\n- NVIDIA Canary\n- Audio denoising techniques",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "v4gs7gxuqw",
   "source": "## Import Required Libraries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "q3n58e98ph",
   "source": "# Standard libraries\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Audio processing libraries\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom scipy.signal import butter, filtfilt\n\n# NeMo ASR\nimport nemo.collections.asr as nemo_asr\nfrom nemo.collections.speechlm2.models import SALM\n\n# Whisper ASR\nimport whisper\n\n# Custom models (from your project)\ntry:\n    from models import separate_fast, dnsmos, whisper_asr, silero_vad\n    print(\"Custom models imported successfully\")\nexcept ImportError as e:\n    print(f\"Note: Custom models not found - {e}\")\n    print(\"You may need to run this from the correct directory\")\n\n# Jupyter display\nimport IPython.display as ipd\n\nprint(\"All libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vu6r9hcsym",
   "source": "## 1. NeMo ASR (NVIDIA Parakeet TDT)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0b0c7",
   "metadata": {},
   "outputs": [],
   "source": "# Load NeMo ASR model (Parakeet TDT 0.6B)\nasr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v2\")"
  },
  {
   "cell_type": "markdown",
   "id": "c99zoi2ky5f",
   "source": "### Download sample audio file",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6199111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-02 02:04:15--  https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\n",
      "Resolving dldata-public.s3.us-east-2.amazonaws.com (dldata-public.s3.us-east-2.amazonaws.com)... 52.219.94.146, 3.5.133.198, 52.219.177.162, ...\n",
      "Connecting to dldata-public.s3.us-east-2.amazonaws.com (dldata-public.s3.us-east-2.amazonaws.com)|52.219.94.146|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 237964 (232K) [audio/wav]\n",
      "Saving to: ‘2086-149220-0033.wav’\n",
      "\n",
      "2086-149220-0033.wa 100%[===================>] 232.39K   254KB/s    in 0.9s    \n",
      "\n",
      "2025-12-02 02:04:17 (254 KB/s) - ‘2086-149220-0033.wav’ saved [237964/237964]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cwv5yihofg",
   "source": "### Transcribe with NeMo",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b539d",
   "metadata": {},
   "outputs": [],
   "source": "# Transcribe audio file\noutput = asr_model.transcribe(['2086-149220-0033.wav'])\nprint(\"Transcription:\", output[0].text)"
  },
  {
   "cell_type": "markdown",
   "id": "nssd3q15uhl",
   "source": "### Detailed output (Hypothesis object)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76af41f",
   "metadata": {},
   "outputs": [],
   "source": "# View full output details\nprint(output[0])"
  },
  {
   "cell_type": "markdown",
   "id": "myn88ybe6wp",
   "source": "## 2. Custom Whisper ASR (with VAD)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5554967",
   "metadata": {},
   "outputs": [],
   "source": "# Load custom Whisper ASR model with VAD\n# Note: This requires the custom whisper_asr module from your project\nasr_model_whisper = whisper_asr.load_asr_model(\n    \"large-v3\",\n    \"cuda\",\n    compute_type=\"float16\",\n    threads=4,\n    language=\"en\",\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadbe84",
   "metadata": {},
   "outputs": [],
   "source": "# Note: This custom model requires vad_segments parameter\n# Example (uncomment and provide VAD segments):\n# transcribe = asr_model_whisper.transcribe('2086-149220-0033.wav', vad_segments=your_vad_segments)\nprint(\"This custom Whisper model requires VAD segments as input.\")"
  },
  {
   "cell_type": "markdown",
   "id": "sic9ah45dp",
   "source": "## 3. NVIDIA Canary (Speech Language Model)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2837e9",
   "metadata": {},
   "outputs": [],
   "source": "# Set GPU device\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n# Load NVIDIA Canary model\nmodel = SALM.from_pretrained('nvidia/canary-qwen-2.5b')"
  },
  {
   "cell_type": "markdown",
   "id": "8plnqm7y33w",
   "source": "### Generate transcription with Canary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a1df4",
   "metadata": {},
   "outputs": [],
   "source": "# Generate transcription using Canary model\nanswer_ids = model.generate(\n    prompts=[\n        [{\"role\": \"user\", \"content\": f\"Transcribe the following: {model.audio_locator_tag}\", \"audio\": [\"2086-149220-0033.wav\"]}]\n    ],\n    max_new_tokens=128,\n)\ntranscription = model.tokenizer.ids_to_text(answer_ids[0].cpu())\nprint(\"Canary Transcription:\", transcription)"
  },
  {
   "cell_type": "markdown",
   "id": "auale8ywrs8",
   "source": "## 4. OpenAI Whisper (Standard)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714465a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/fr20tb/kyudan/miniforge3/envs/dataset/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "100%|██████████████████████████████████████| 2.88G/2.88G [00:11<00:00, 260MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing...\n",
      "Text:  You're the emcee on the show, Ira. Oh, great. Ira, are you Ira?\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# 파일 경로 변수 지정\n",
    "file_path = \"/mnt/ddn/kyudan/Audio-data-centric/podcast-pipeline/data/test/_final/-sepreformer-True-vad-True-diaModel-dia3-initPrompt-False-merge_gap-2.0-seg_th-0.11-cl_min-11-cl-th-0.5-LLM-case_0/test_english_with_overlap/test_english_with_overlap/00003_SPEAKER_01.mp3\"\n",
    "\n",
    "# 모델 로드 (large-v3)\n",
    "# GPU가 있으면 자동으로 cuda를 사용하지만, 명시적으로 device=\"cuda\"를 넣어도 됩니다.\n",
    "model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "print(\"Transcribing...\")\n",
    "result = model.transcribe(file_path)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Text: {result['text']}\")\n",
    "\n",
    "# 필요하면 결과 저장\n",
    "# with open(\"result.txt\", \"w\") as f:\n",
    "#     f.write(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238bb5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "id": "k9cslmeqvt",
   "source": "# 원하는 방법의 결과를 파일로 저장 (옵션)\noutput_method = 'spectral_gate'  # 또는 'bandpass', 'wiener'\noutput_path = f\"denoised_{output_method}.wav\"\n\nsf.write(output_path, denoised_audios[output_method], sr)\nprint(f\"Denoised audio saved to: {output_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5wq0x9g92q9",
   "source": "# 각 방법별 결과 재생\nprint(\"=== Spectral Gate Denoising ===\")\nipd.display(ipd.Audio(denoised_audios['spectral_gate'], rate=sr))\n\nprint(\"\\n=== Bandpass Filter Denoising ===\")\nipd.display(ipd.Audio(denoised_audios['bandpass'], rate=sr))\n\nprint(\"\\n=== Wiener Filter Denoising ===\")\nipd.display(ipd.Audio(denoised_audios['wiener'], rate=sr))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "065zkzgqlxbo",
   "source": "def simple_denoising(audio, sr, method='spectral_gate'):\n    \"\"\"\n    간단한 디노이징 알고리즘\n    \n    Parameters:\n    - audio: 오디오 신호 (numpy array)\n    - sr: 샘플링 레이트\n    - method: 'spectral_gate', 'bandpass', 'wiener'\n    \n    Returns:\n    - denoised_audio: 디노이즈된 오디오\n    \"\"\"\n    \n    if method == 'spectral_gate':\n        # 스펙트럴 게이팅 방법\n        print(\"Applying spectral gating denoising...\")\n        \n        # STFT 수행\n        D = librosa.stft(audio)\n        magnitude, phase = np.abs(D), np.angle(D)\n        \n        # 노이즈 프로파일 추정 (처음 0.5초를 노이즈로 가정)\n        noise_frames = int(0.5 * sr / 512)\n        noise_profile = np.median(magnitude[:, :noise_frames], axis=1, keepdims=True)\n        \n        # 스펙트럴 게이팅\n        threshold = 2.0  # 노이즈 프로파일의 2배를 임계값으로 설정\n        mask = magnitude > (noise_profile * threshold)\n        \n        # 마스크 적용\n        magnitude_denoised = magnitude * mask\n        \n        # ISTFT로 복원\n        D_denoised = magnitude_denoised * np.exp(1j * phase)\n        denoised = librosa.istft(D_denoised)\n        \n    elif method == 'bandpass':\n        # 밴드패스 필터 (음성 주파수 대역: 80-8000 Hz)\n        print(\"Applying bandpass filter denoising...\")\n        \n        nyquist = sr / 2\n        low = 80 / nyquist\n        high = 8000 / nyquist\n        \n        b, a = butter(5, [low, high], btype='band')\n        denoised = filtfilt(b, a, audio)\n        \n    elif method == 'wiener':\n        # Wiener 필터링 (간단한 버전)\n        print(\"Applying Wiener filter denoising...\")\n        \n        # STFT\n        D = librosa.stft(audio)\n        magnitude = np.abs(D)\n        phase = np.angle(D)\n        \n        # 노이즈 추정\n        noise_frames = int(0.5 * sr / 512)\n        noise_power = np.mean(magnitude[:, :noise_frames] ** 2, axis=1, keepdims=True)\n        \n        # Wiener 필터\n        signal_power = magnitude ** 2\n        wiener_gain = np.maximum(1 - noise_power / (signal_power + 1e-10), 0.1)\n        \n        magnitude_denoised = magnitude * wiener_gain\n        \n        # ISTFT\n        D_denoised = magnitude_denoised * np.exp(1j * phase)\n        denoised = librosa.istft(D_denoised)\n    \n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n    \n    return denoised\n\n# 여러 방법으로 디노이징\nmethods = ['spectral_gate', 'bandpass', 'wiener']\ndenoised_audios = {}\n\nfor method in methods:\n    print(f\"\\n--- Method: {method} ---\")\n    denoised = simple_denoising(audio.copy(), sr, method=method)\n    denoised_audios[method] = denoised\n    print(f\"Denoised audio length: {len(denoised)} samples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "u9ehysklei",
   "source": "import numpy as np\nimport librosa\nimport soundfile as sf\nfrom scipy.signal import butter, filtfilt\nimport IPython.display as ipd\n\n# 파일 경로\nfile_path = \"/mnt/ddn/kyudan/Audio-data-centric/podcast-pipeline/data/test/_final/-sepreformer-True-vad-True-diaModel-dia3-initPrompt-False-merge_gap-2.0-seg_th-0.11-cl_min-11-cl_th-0.5-LLM-case_0/test_english_with_overlap/test_english_with_overlap/00003_SPEAKER_01.mp3\"\n\n# 오디오 로드\nprint(\"Loading audio...\")\naudio, sr = librosa.load(file_path, sr=None)\nprint(f\"Sample rate: {sr} Hz, Duration: {len(audio)/sr:.2f} seconds\")\n\n# 원본 재생\nprint(\"\\n=== Original Audio ===\")\nipd.display(ipd.Audio(audio, rate=sr))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}