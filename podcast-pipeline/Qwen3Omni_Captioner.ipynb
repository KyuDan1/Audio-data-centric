{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac61a469",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LLM' from 'vllm' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Qwen3OmniMoeProcessor\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqwen_omni_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m process_mm_info\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'LLM' from 'vllm' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import Qwen3OmniMoeProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # vLLM engine v1 not supported yet\n",
    "    os.environ['VLLM_USE_V1'] = '0'\n",
    "\n",
    "    MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Captioner\"\n",
    "\n",
    "    llm = LLM(\n",
    "            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,\n",
    "            tensor_parallel_size=torch.cuda.device_count(),\n",
    "            limit_mm_per_prompt={'audio': 1},\n",
    "            max_num_seqs=8,\n",
    "            max_model_len=32768,\n",
    "            seed=1234,\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        max_tokens=16384,\n",
    "    )\n",
    "\n",
    "    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/caption2.mp3\"}\n",
    "            ], \n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    audios, _, _ = process_mm_info(messages, use_audio_in_video=False)\n",
    "\n",
    "    inputs = {\n",
    "        'prompt': text,\n",
    "        'multi_modal_data': {},\n",
    "    }\n",
    "\n",
    "    if audios is not None:\n",
    "        inputs['multi_modal_data']['audio'] = audios\n",
    "\n",
    "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
    "\n",
    "    print(outputs[0].outputs[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872fb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3omnicaption",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
